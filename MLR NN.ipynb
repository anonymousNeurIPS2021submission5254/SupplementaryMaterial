{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author Benjamin RIU riubenjamin@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a NVIDIA 2080Ti (11.G of VRAM), should also work with a 2080 (8.G of VRAM):\n",
    "MAX_GPU_MATRIX_WIDTH = int(4096) #Max width of any matrix; scales quadraticaly with VRAM\n",
    "MAX_GPU_NETWORK_DEPTH = int(6) #Max depth of any network; scales linearly with VRAM\n",
    "#For same VRAM size, you can pick a different tradeoff between those two.\n",
    "#You could also increase matrix width or network depth beyond these values if you reduce either your batch size or network width accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T10:03:32.670690Z",
     "start_time": "2021-05-19T10:03:32.612947Z"
    },
    "code_folding": [
     62,
     158,
     164
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import torch as torch\n",
    "\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, roc_auc_score, accuracy_score\n",
    "from scipy.special import expit as logistic_func\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import time\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#USE GPU\n",
    "CUDA = True\n",
    "if torch.cuda.is_available() and CUDA: dev = \"cuda:0\"\n",
    "else: dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "#quick shortcuts\n",
    "def n2t(array):\n",
    "    #numpy array to torch Tensor\n",
    "    if type(array) == torch.Tensor: return array\n",
    "    return torch.tensor(array).to(device).float()\n",
    "def t2n(tensor):\n",
    "    #torch Tensor to numpy array\n",
    "    if type(tensor) == np.ndarray: return tensor\n",
    "    return tensor.detach().to(\"cpu\").numpy()\n",
    "def n2f(array):\n",
    "    #numpy array to float\n",
    "    if type(array) == torch.Tensor: array = t2n(array)\n",
    "    if type(array) == float: return array\n",
    "    else: return np.array(array).reshape(-1)[0]\n",
    "def f2s(value, length = 8, delimiter = \" |\"):\n",
    "    #float to string with fixed length\n",
    "    if type(value) == str:\n",
    "        return \" \" * (length - len(value)) + value[:length] + delimiter\n",
    "    else:\n",
    "        return (\"%.\" +str(length-6)+\"e\") % float(value) + delimiter\n",
    "\n",
    "class BaseMLRNN(BaseEstimator, metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def __init__(self, #see subclasses for default values\n",
    "        depth,#number of hidden layers, int\n",
    "        width, #number of neurons on hidden layers, int\n",
    "        activation_function, #a torch function, str\n",
    "        loss_function, #set by the subclass\n",
    "                 \n",
    "        optimizer, #a torch optimizer, str\n",
    "        learning_rate, #float\n",
    "        batch_size, #if None or False: full batch, if int number of samples, if float share of samples\n",
    "        max_iter, #iterations, not epochs, int\n",
    "        max_runtime, #unprecise, float or int\n",
    "                 \n",
    "        validation_fraction, #if None or False: no validation, if int number of samples, if float share of samples\n",
    "        should_stratify, #validation split strategy, bool\n",
    "        early_stopping_criterion, #either \"loss\" or \"validation\", str\n",
    "        convergence_tol, #if None or False: always max_iter, else float \n",
    "        divergence_tol, #if None or False: always max_iter, else float \n",
    "        \n",
    "        ridge_init, #if None or False: regular FFNN, if int or float lambda initial value, if \"max_variation\" or \"min_value\" grid-search\n",
    "        n_permut, #if int number of permutations, if None or False no permutations\n",
    "        label_noise_scale,#if float dithering white noise standard-deviation, if None or False no gaussian dithering\n",
    "        target_rotation_scale,#if float dithering structured noise standard-deviation, if None or False no structured noise dithering\n",
    "        \n",
    "        random_state, #scikit-learn random state, will also set torch generator using a different seed\n",
    "        verbose #if False mute, if True print at each iteration, if int print if iter%verbose == 0\n",
    "                ):\n",
    "        \n",
    "        self.depth  = depth\n",
    "        self.width  = width\n",
    "        self.activation_function  = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer  = optimizer\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.batch_size  = batch_size\n",
    "        self.max_iter  = max_iter\n",
    "        self.max_runtime  = max_runtime\n",
    "        self.validation_fraction  = validation_fraction\n",
    "        self.should_stratify = should_stratify\n",
    "        self.early_stopping_criterion  = early_stopping_criterion\n",
    "        self.convergence_tol  = convergence_tol\n",
    "        self.divergence_tol  = divergence_tol\n",
    "        self.ridge_init  = ridge_init\n",
    "        self.n_permut  = n_permut\n",
    "        self.label_noise_scale  = label_noise_scale\n",
    "        self.target_rotation_scale = target_rotation_scale\n",
    "        self.random_state  = random_state\n",
    "        self.verbose  = verbose\n",
    "        \n",
    "    def _init_valid(self, X, y):\n",
    "\n",
    "        if self.validation_fraction in [False, None]:\n",
    "            self.validation = False\n",
    "            return X, None, y, None\n",
    "        else:\n",
    "            def roc_with_proba(y, probas): return roc_auc_score(y, probas[:,-1])\n",
    "            self.valid_metric = roc_with_proba if is_classifier(self) else r2_score\n",
    "            self.valid_func = self.predict_proba if is_classifier(self) else self.predict\n",
    "            self.validation = True\n",
    "            if self.should_stratify:\n",
    "                if is_classifier(self): stratify = y \n",
    "                else: \n",
    "                    stratify = self._stratify_continuous_target(y)\n",
    "            else: stratify = None\n",
    "            X, X_valid, y, y_valid = train_test_split(\n",
    "                X, y, random_state=self._random_state,\n",
    "                test_size=self.validation_fraction,\n",
    "                stratify=stratify)\n",
    "            return X, n2t(X_valid), y, y_valid\n",
    "        \n",
    "\n",
    "    def _stratify_continuous_target(self, y):\n",
    "        from sklearn.tree import DecisionTreeRegressor as tree_binarizer\n",
    "        MAX_TRAINING_SAMPLES, MAX_TREE_SIZE = int(1e4), 50 #CPU memory and runtime safeguards\n",
    "        tree_binarizer_params = {\"criterion\":'friedman_mse', \n",
    "                   \"splitter\":'best', \n",
    "                   \"max_depth\":None, \n",
    "                   \"min_samples_split\":2, \n",
    "                   \"min_weight_fraction_leaf\":0.0, \n",
    "                   \"max_features\":None,  \n",
    "                   \"min_impurity_decrease\":0.2, \n",
    "                   \"min_impurity_split\":None, \n",
    "                   \"ccp_alpha\":0.0}\n",
    "        \n",
    "        tree_size = min(int(np.sqrt(len(y))), MAX_TREE_SIZE)\n",
    "        fit_sample_size = min(len(y), MAX_TRAINING_SAMPLES)\n",
    "        tree_binarizer_params[\"random_state\"] = self._random_state\n",
    "        tree_binarizer_params[\"max_leaf_nodes\"] = tree_size\n",
    "        tree_binarizer_params[\"min_samples_leaf\"] = tree_size\n",
    "        return tree_binarizer(**tree_binarizer_params).fit(y[:fit_sample_size].reshape((-1,1)), y[:fit_sample_size]).apply(y.reshape((-1,1)))\n",
    "        \n",
    "    def _valid_score(self, ):\n",
    "        return self.valid_metric(self.y_valid, self.valid_func(self.X_valid))\n",
    "    \n",
    "    def _init_batch(self, n_samples):\n",
    "        if type(self.batch_size) == float:\n",
    "            self.batch_length = min(MAX_GPU_MATRIX_WIDTH, int(n_samples * self.batch_size))\n",
    "            self.batch_learning = True\n",
    "        elif type(self.batch_size) == int:\n",
    "            self.batch_length = min(MAX_GPU_MATRIX_WIDTH, self.batch_size, n_samples)\n",
    "            self.batch_learning = True\n",
    "        elif n_samples > MAX_GPU_MATRIX_WIDTH:\n",
    "            self.batch_length = int(MAX_GPU_MATRIX_WIDTH)\n",
    "            self.batch_learning = True\n",
    "        else:\n",
    "            self.batch_learning = False\n",
    "        if self.batch_learning:\n",
    "            self.n_batches = int(n_samples / self.batch_length) \n",
    "            \n",
    "    def _init_termination_criterion(self,):\n",
    "        self.check_convergence = self.convergence_tol not in [False, None]\n",
    "        self.check_divergence = self.divergence_tol not in [False, None]\n",
    "        self.early_stopping = self.early_stopping_criterion not in [False, None]\n",
    "        self.sign_criterion = -1. if self.early_stopping_criterion == \"validation\" else 1.\n",
    "            \n",
    "    def _init_MLR(self, ):\n",
    "        self.ridge_output = self.ridge_init not in [False, None]\n",
    "        self.add_MLR = self.ridge_output and self.n_permut not in [False, None, 0, 0.]   \n",
    "        \n",
    "    def _init_label_noise(self, ):\n",
    "        self.add_label_noise = self.label_noise_scale not in [False, None, 0, 0.] \n",
    "        \n",
    "    def _init_target_rotation(self, ):\n",
    "        self.rotate_target = self.target_rotation_scale not in [False, None, 0, 0.] \n",
    "        \n",
    "    def _init_hidden_weights(self, ):\n",
    "        self.hidden_layers = []\n",
    "        for layer in range(self.depth + 1 - int(self.ridge_output)):\n",
    "            fan_in = self.n_features if layer == 0 else self.width\n",
    "            fan_out = 1 if layer == self.depth else self.width\n",
    "            factor = 2 if self.activation_function == \"logistic\" else 6 \n",
    "            if self.activation_function == \"relu\": factor = factor * 2\n",
    "            init_bound = np.sqrt(factor/(fan_in + fan_out))\n",
    "            W = init_bound * (torch.rand((fan_in, fan_out), generator = self.torch_random_state, device = device)* 2. - 1.) \n",
    "            B = torch.zeros(size = (1,fan_out), device = device)\n",
    "            self.hidden_layers.append([torch.nn.Parameter(weights) for weights in (W,B)])\n",
    "            del W,B\n",
    "        params = [weight for layer in self.hidden_layers for weight in layer]\n",
    "        if self.ridge_output: self.hidden_layers.append([None, torch.zeros(size = (1,), device = device)])\n",
    "        return params\n",
    "    \n",
    "    def _grid_search_ridge_coef(self, datas):\n",
    "        GRID_START, GRID_END, GRID_SIZE = 1e-1, 1e4, 11\n",
    "        candidates, losses = np.geomspace(GRID_START, GRID_END, GRID_SIZE),np.zeros(GRID_SIZE)\n",
    "        with torch.no_grad():\n",
    "            activation = self._forward_pass(datas[\"input\"])\n",
    "            target = self._add_noise(datas[\"target\"])\n",
    "            activation_dot_target = activation.transpose(1,0) @ self._scale_target(target)\n",
    "            activation_dot_target_permuted = None\n",
    "            if self.add_MLR: \n",
    "                target_permuted = self._add_noise(datas[\"target_permuted\"])\n",
    "                activation_dot_target_permuted = activation.transpose(1,0) @ self._scale_target(target_permuted)\n",
    "            for i,candidate in enumerate(candidates):\n",
    "                activation_dot_inv_mat = activation @ self._get_inv_mat(activation, n2t(np.log(candidate)), only_inversion = True)\n",
    "                loss = self._compute_loss(activation_dot_inv_mat @ activation_dot_target, target)\n",
    "                if self.add_MLR: loss += self._compute_MLR_penalty(activation_dot_inv_mat @ activation_dot_target_permuted, target_permuted)\n",
    "                losses[i] = n2f(t2n(loss))\n",
    "            \n",
    "        if self.ridge_init == \"max_variation\":\n",
    "            return np.geomspace(GRID_START, GRID_END, GRID_SIZE-1)[np.argmax(losses[1:] - losses[:-1])]\n",
    "        else:\n",
    "            return candidates[np.argmin(losses)]\n",
    "    \n",
    "    def _init_ridge_coef(self, datas):\n",
    "        if self.ridge_output:\n",
    "\n",
    "            if type(self.ridge_init) in [float, int] or isinstance(self.ridge_init, np.number):\n",
    "                ridge_coef = self.ridge_init\n",
    "                \n",
    "            elif self.ridge_init in [\"min_value\", \"max_variation\"]:\n",
    "                ridge_coef = self._grid_search_ridge_coef(datas)\n",
    "            else:\n",
    "                ridge_coef = 1.\n",
    "                \n",
    "            self.ridge_coef = torch.nn.Parameter(n2t(np.log(ridge_coef)))\n",
    "            return [self.ridge_coef]     \n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    def _init_record(self, ):\n",
    "        self.record = {}\n",
    "        self.record[\"loss\"] = []\n",
    "        self.record[\"time\"] = []\n",
    "        if self.validation:\n",
    "            self.record[\"validation\"] = []\n",
    "        if self.ridge_output:\n",
    "            self.record[\"lambda\"] = []\n",
    "        if self.add_MLR:\n",
    "            self.record[\"mlr\"] = []\n",
    "            \n",
    "    def _init_intercept(self, target):\n",
    "        if self.add_MLR:\n",
    "            with torch.no_grad():\n",
    "                pred = target.mean() * torch.ones(target.shape, device = device)\n",
    "                self.intercept = self._compute_loss(self._scale_target(pred), target).detach()\n",
    "            \n",
    "    def _initialize(self, X, y):\n",
    "        self.init_time = time.time()\n",
    "        self._random_state = check_random_state(self.random_state)\n",
    "        self.torch_random_state = torch.Generator(device=device).manual_seed(int(self._random_state.uniform(0,2**31)))\n",
    "        self.print_record  = self.verbose not in [0, False]\n",
    "        X, self.X_valid, y, self.y_valid = self._init_valid(X, y)\n",
    "        n_samples, self.n_features = X.shape\n",
    "        self._init_termination_criterion()\n",
    "        self._init_batch(n_samples)\n",
    "        self._init_MLR()\n",
    "        self._init_record()\n",
    "        self._init_label_noise(), self._init_target_rotation()\n",
    "        self.act_func = getattr(torch, self.activation_function)\n",
    "        self.loss_func = getattr(torch.nn,self.loss_function)(reduction='none')\n",
    "        \n",
    "        datas = self._init_data(X,y)\n",
    "        self._init_intercept(datas[\"target\"])\n",
    "        params = self._init_hidden_weights()\n",
    "        params = params + self._init_ridge_coef(datas)\n",
    "        self.optimizer_instance  = getattr(torch.optim, self.optimizer)(lr = self.learning_rate, params = params)\n",
    "        del params\n",
    "        self.current_iter = 0\n",
    "        self.init_time = time.time() - self.init_time\n",
    "        return X, y, datas\n",
    "    \n",
    "    def _reduce_loss(self, point_wise_loss):\n",
    "        if self.loss_function == \"MSE\":\n",
    "            return torch.sqrt(point_wise_loss.mean(dim = 0))\n",
    "        else: return point_wise_loss.mean(dim = 0)\n",
    "        \n",
    "        \n",
    "    def _init_data(self, X,y):\n",
    "        datas = {\"X\":X, \"y\":y}\n",
    "        if self.add_MLR:\n",
    "            datas[\"y_permuted\"] = self._permut_label(y)\n",
    "            \n",
    "        if self.batch_learning:\n",
    "            self._generate_batch(datas[\"X\"].shape[0])\n",
    "            datas = self._update_data(datas)\n",
    "        else: \n",
    "            datas[\"input\"] = n2t(datas[\"X\"])\n",
    "            datas[\"target\"] = n2t(datas[\"y\"])\n",
    "            if self.add_MLR: datas[\"target_permuted\"] = n2t(datas[\"y_permuted\"])\n",
    "        return datas\n",
    "            \n",
    "        \n",
    "    def _generate_batch(self, n_samples):\n",
    "        shuffled_indexes = shuffle(np.arange(n_samples),random_state=self._random_state)\n",
    "        self.batches = [shuffled_indexes[batch_i * self.batch_length: (batch_i + 1) * self.batch_length] for batch_i in range(self.n_batches)]\n",
    "        \n",
    "    def _update_data(self, datas):\n",
    "        if self.batch_learning:\n",
    "            if len(self.batches) == 0:\n",
    "                self._generate_batch(datas[\"X\"].shape[0])\n",
    "            batch_indexes = self.batches.pop(0)\n",
    "            datas[\"input\"] = n2t(datas[\"X\"][batch_indexes])\n",
    "            datas[\"target\"] = n2t(datas[\"y\"][batch_indexes])\n",
    "            if self.add_MLR: datas[\"target_permuted\"] = n2t(datas[\"y_permuted\"][batch_indexes])\n",
    "        return datas\n",
    "    \n",
    "    def _permut_label(self, y):\n",
    "        y_index = np.arange(len(y))\n",
    "        return np.concatenate([y[shuffle(y_index,random_state=self._random_state)].reshape((-1,1)) for permut in range(self.n_permut)], axis=1)\n",
    "        \n",
    "    def _forward_propagate(self, datas):\n",
    "        activation = datas[\"input\"]\n",
    "        target = self._add_noise(datas[\"target\"])\n",
    "        activation = self._forward_pass(activation)\n",
    "        \n",
    "        if self.ridge_output:\n",
    "            inv_mat = self._get_inv_mat(activation, self.ridge_coef)\n",
    "            beta = inv_mat @ self._scale_target(target)\n",
    "            self.hidden_layers[-1][0] = beta\n",
    "            pred = activation @ beta\n",
    "            if self.rotate_target:\n",
    "                projection = activation @ inv_mat\n",
    "                pred, target = self._rotate_pred(projection, pred, target)\n",
    "        else:\n",
    "            pred = activation.reshape(-1)\n",
    "        loss = self._compute_loss(pred, target)\n",
    "        self.record[\"loss\"].append(n2f(t2n(loss)))\n",
    "        if self.add_MLR:\n",
    "            target_permutation = self._add_noise(datas[\"target_permuted\"])\n",
    "            if not self.rotate_target: projection = activation @ inv_mat\n",
    "            pred_permutation = projection @ self._scale_target(target_permutation)\n",
    "            if self.rotate_target:\n",
    "                pred_permutation, target_permutation = self._rotate_pred(projection, pred_permutation, target_permutation)\n",
    "            permut_loss = self._compute_MLR_penalty(pred_permutation, target_permutation)\n",
    "            self.record[\"mlr\"].append(n2f(t2n(permut_loss)))\n",
    "            loss += permut_loss\n",
    "        if self.validation: self.record[\"validation\"].append(self._valid_score())\n",
    "        if self.early_stopping: self._save_weights()\n",
    "        if self.ridge_output:\n",
    "            self.record[\"lambda\"].append(np.exp(n2f(t2n(self.ridge_coef))))\n",
    "        self.record[\"time\"].append(time.time() - self.current_time)\n",
    "        self.current_time = time.time()\n",
    "        if self.print_record: \n",
    "            if self.current_iter == 0:\n",
    "                print(\"| \"+f2s(\"iter\"), *map(f2s, self.record.keys()))\n",
    "            if self.current_iter % int(self.verbose) == 0:\n",
    "                print(\"| \"+f2s(str(self.current_iter)), *map(lambda value : f2s(value[-1]), self.record.values()))\n",
    "                      \n",
    "        return loss\n",
    "        \n",
    "    def _add_noise(self, target):\n",
    "        if self.add_label_noise:\n",
    "            return target  + torch.normal(0., self.label_noise_scale, size = target.shape, generator = self.torch_random_state, device = device)\n",
    "        else:\n",
    "            return target\n",
    "        \n",
    "    def _rotate_pred(self, projection, prediction, target):\n",
    "        if self.rotate_target:\n",
    "            epsilon = torch.normal(0.,self.target_rotation_scale, size = target.shape ,generator = self.torch_random_state, device = device)\n",
    "            complementary = epsilon - projection @ epsilon\n",
    "            return (prediction + target)/2 + complementary, target\n",
    "        else:\n",
    "            return prediction, target\n",
    "        \n",
    "    def _scale_target(self, target):\n",
    "        if is_classifier(self):\n",
    "            return target * 2. - 1.\n",
    "        else:\n",
    "            return target\n",
    "        \n",
    "    def _forward_pass(self, activation):\n",
    "        for layer,(W,B) in enumerate(self.hidden_layers[:self.depth + 1 - int(self.ridge_output)]):\n",
    "            activation = activation @ W + B\n",
    "            if layer < self.depth: activation = self.act_func(activation)\n",
    "        return activation\n",
    "    \n",
    "    def _get_inv_mat(self, activation, ridge_coef, only_inversion = False):\n",
    "        diag = torch.diag(torch.ones(activation.shape[1],device = device) * torch.exp(ridge_coef))\n",
    "        inversed = torch.inverse((activation.transpose(1,0) @ activation) + diag)\n",
    "        if only_inversion: return inversed\n",
    "        else: return inversed @ activation.transpose(1,0)\n",
    "      \n",
    "    def _compute_loss(self, pred, target):\n",
    "        return self._reduce_loss(self.loss_func(pred, target)).mean()\n",
    "        \n",
    "    def _compute_MLR_penalty(self, pred, target):\n",
    "        return torch.abs(self.intercept - self._reduce_loss(self.loss_func(pred, target))).mean()\n",
    "        \n",
    "    def _backward_propagate(self, loss):\n",
    "        loss.backward()\n",
    "        self.optimizer_instance.step()\n",
    "        self.optimizer_instance.zero_grad()\n",
    "        self.current_iter = self.current_iter + 1\n",
    "        \n",
    "    def _check_termination(self,):\n",
    "        return self._check_convergence() or self._check_divergence() or self._check_timeout() or self.current_iter >= self.max_iter\n",
    "    def _check_convergence(self, ):\n",
    "        if self.check_convergence:\n",
    "            return np.abs(np.min(self.record[\"loss\"][:-1]) - self.record[\"loss\"][-1]) < self.convergence_tol\n",
    "        else: return False\n",
    "    def _check_divergence(self, ): \n",
    "        if self.check_divergence:\n",
    "            return self.record[\"loss\"][-1] > self.divergence_tol\n",
    "        else: return False\n",
    "    def _check_timeout(self, ):\n",
    "        return self.max_runtime < self.init_time + np.sum(self.record[\"time\"])\n",
    "    def _save_weights(self,):\n",
    "        if self.current_iter == 0:\n",
    "            self.best_iter = self.current_iter\n",
    "        elif self.record[self.early_stopping_criterion][-1] * self.sign_criterion < self.record[self.early_stopping_criterion][self.best_iter] * self.sign_criterion:\n",
    "            self.best_iter = self.current_iter\n",
    "            del self.saved_hidden_layers\n",
    "        if self.current_iter == self.best_iter:\n",
    "            saved_weights_device = device if self.depth <= MAX_GPU_NETWORK_DEPTH/2 else torch.device(\"cpu\")\n",
    "            self.saved_hidden_layers = [[torch.clone(weights).detach().to(saved_weights_device) for weights in couple] for couple in self.hidden_layers]\n",
    "    def _load_weights(self,):\n",
    "        self.hidden_layers = [[weights.to(device) for weights in couple] for couple in self.saved_hidden_layers]\n",
    "        del self.saved_hidden_layers\n",
    "        \n",
    "    def _release_train_memory(self):\n",
    "        if self.validation: del self.X_valid, self.y_valid\n",
    "        if self.ridge_output: del self.ridge_coef\n",
    "        del self.optimizer_instance\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "    def _forward_pass_fast(self, activation):\n",
    "        with torch.no_grad():\n",
    "            for layer,(W,B) in enumerate(self.hidden_layers):\n",
    "                activation = activation @ W + B\n",
    "                if layer < self.depth: activation = self.act_func(activation)\n",
    "            del W,B\n",
    "        return activation\n",
    "    \n",
    "    def _predict_hidden(self, X):\n",
    "        if X.shape[0] <= MAX_GPU_MATRIX_WIDTH:\n",
    "            return t2n(self._forward_pass_fast(n2t(X)))\n",
    "        else:\n",
    "            return np.concatenate([ self._predict_hidden(X[:MAX_GPU_MATRIX_WIDTH]), self._predict_hidden(X[MAX_GPU_MATRIX_WIDTH:])])\n",
    " \n",
    "    \n",
    "    def _fit(self, X, y, incremental=False):\n",
    "        if incremental:\n",
    "            datas = self._init_data(X,y)\n",
    "        else:\n",
    "            X, y, datas = self._initialize(X, y)\n",
    "            \n",
    "        self.current_time = time.time()\n",
    "        loss = self._forward_propagate(datas)\n",
    "        while not self._check_termination():\n",
    "            self._backward_propagate(loss)\n",
    "            del loss\n",
    "            datas = self._update_data(datas)\n",
    "            loss = self._forward_propagate(datas)\n",
    "            \n",
    "        del loss\n",
    "        if self.early_stopping: self._load_weights()\n",
    "        self._release_train_memory()  \n",
    "        return self\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self._fit(X,y, incremental=False)\n",
    "    \n",
    "    def delete_model_weights(self):\n",
    "        del self.hidden_layers\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def partial_fit(self, X, y):\n",
    "        #does not work currently\n",
    "        return self._fit(X,y, incremental=True)\n",
    "    \n",
    "       \n",
    "class MLRNNClassifier(ClassifierMixin, BaseMLRNN):\n",
    "    def __init__(self, *,\n",
    "        depth = 1,\n",
    "        width = 4096,\n",
    "        activation_function = \"relu\",\n",
    "        optimizer = \"Adam\",\n",
    "        learning_rate = 1e-2,\n",
    "        batch_size = False,\n",
    "        max_iter = 50,\n",
    "        max_runtime = 300,       \n",
    "        validation_fraction = 0.2,\n",
    "        should_stratify = True,\n",
    "        early_stopping_criterion = \"validation\",\n",
    "        convergence_tol = False,\n",
    "        divergence_tol = False,\n",
    "        ridge_init = \"max_variation\",\n",
    "        n_permut = 16,\n",
    "        label_noise_scale = None,\n",
    "        target_rotation_scale = 1.,\n",
    "        random_state = None,\n",
    "        verbose = False\n",
    "                ):\n",
    "        super().__init__(\n",
    "        depth  = depth, width  = width, activation_function  = activation_function,\n",
    "        loss_function = 'BCEWithLogitsLoss', optimizer  = optimizer, learning_rate  = learning_rate,\n",
    "        batch_size  = batch_size, max_iter  = max_iter, max_runtime  = max_runtime, validation_fraction  = validation_fraction,\n",
    "        should_stratify= should_stratify, early_stopping_criterion  = early_stopping_criterion, convergence_tol  = convergence_tol, divergence_tol  = divergence_tol,\n",
    "        ridge_init  = ridge_init, n_permut  = n_permut, label_noise_scale  = label_noise_scale,\n",
    "        target_rotation_scale =  target_rotation_scale, random_state  = random_state, verbose  = verbose)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self._predict_hidden(X) >= 0.\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        return self._predict_hidden(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        proba = logistic_func(self._predict_hidden(X))\n",
    "        return np.c_[1.-proba, proba]\n",
    "    \n",
    "class MLRNNRegressor(RegressorMixin, BaseMLRNN):\n",
    "    def __init__(self, *,\n",
    "        depth = 1,\n",
    "        width = 4096,\n",
    "        activation_function = \"relu\",  \n",
    "        optimizer = \"Adam\",\n",
    "        learning_rate = 1e-2,\n",
    "        batch_size = False,\n",
    "        max_iter = 50,\n",
    "        max_runtime = 300,       \n",
    "        validation_fraction = 0.2,\n",
    "        should_stratify = True,\n",
    "        early_stopping_criterion = \"validation\",\n",
    "        convergence_tol = False,\n",
    "        divergence_tol = 1.,\n",
    "        ridge_init = \"max_variation\",\n",
    "        n_permut = 16,\n",
    "        label_noise_scale = 0.03,\n",
    "        target_rotation_scale = 0.5,\n",
    "        random_state = None,\n",
    "        verbose = False\n",
    "                ):\n",
    "        super().__init__(\n",
    "        depth  = depth, width  = width, activation_function  = activation_function,\n",
    "        loss_function = 'MSELoss', optimizer  = optimizer, learning_rate  = learning_rate,\n",
    "        batch_size  = batch_size, max_iter  = max_iter, max_runtime  = max_runtime, validation_fraction  = validation_fraction,\n",
    "        should_stratify = should_stratify, early_stopping_criterion  = early_stopping_criterion, convergence_tol  = convergence_tol, divergence_tol  = divergence_tol,\n",
    "        ridge_init  = ridge_init, n_permut  = n_permut, label_noise_scale  = label_noise_scale,\n",
    "        target_rotation_scale =  target_rotation_scale, random_state  = random_state, verbose  = verbose)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self._predict_hidden(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Test with boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as normalize\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.datasets import load_boston as lb\n",
    "def prepare_dataset(dataset, train_size = 0.8, seed= False):\n",
    "    kwargs = {}\n",
    "    if seed or type(seed)== type(0):\n",
    "        kwargs[\"random_state\"] = seed\n",
    "        np.random.seed(seed)\n",
    "    X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        \n",
    "    X = normalize().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = tts(X, y, train_size = train_size, **kwargs)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def prepare_boston(train_size = 0.8, seed= 0):\n",
    "    X,y = lb(return_X_y=True)\n",
    "    y = (y - y.mean())/y.std()\n",
    "    mat = np.concatenate([X, y.reshape((-1,1))], axis= -1)\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset(mat, train_size = train_size,seed=seed)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "dataset_loader = prepare_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rs = 0\n",
    "verbose = False\n",
    "X_train, X_test, y_train, y_test = prepare_boston(seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLRNNRegressor(max_iter = 200, learning_rate = 1e-3, depth = 2, width = 1024, random_state=rs, verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLRNNRegressor(depth=2, learning_rate=0.001, max_iter=200, random_state=0,\n",
       "               width=1024)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7947821597089563\n"
     ]
    }
   ],
   "source": [
    "print(reg.score(X_test, y_test)) #lucky on first try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_record = reg.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.delete_model_weights()  #else gpu memory leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7746811684410286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "print(RF(random_state = seed).fit(X_train,y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture ZOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "common_parameters = {\"max_runtime\" : 900, \"width\" : 1024}\n",
    "MLR1_parameters = copy.deepcopy(common_parameters)\n",
    "MLR1_parameters[\"depth\"] = 1\n",
    "MLR1_parameters[\"learning_rate\"] = 1e-2\n",
    "MLR1_parameters[\"max_iter\"] = 200\n",
    "\n",
    "MLR2_parameters = copy.deepcopy(common_parameters)\n",
    "MLR2_parameters[\"depth\"] = 2\n",
    "MLR2_parameters[\"learning_rate\"] = 1e-3\n",
    "MLR2_parameters[\"max_iter\"] = 200\n",
    "\n",
    "MLR3_parameters = copy.deepcopy(common_parameters)\n",
    "MLR3_parameters[\"depth\"] = 3\n",
    "MLR3_parameters[\"learning_rate\"] = 1e-3 /3\n",
    "MLR3_parameters[\"max_iter\"] = 400\n",
    "\n",
    "MLR4_parameters = copy.deepcopy(common_parameters)\n",
    "MLR4_parameters[\"depth\"] = 4\n",
    "MLR4_parameters[\"learning_rate\"] = 1e-4\n",
    "MLR4_parameters[\"max_iter\"] = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 10 #number of NN\n",
    "architecture = MLR2_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7863358963693238\n"
     ]
    }
   ],
   "source": [
    "prediction = 0.\n",
    "for rs in range(rep):\n",
    "    model = MLRNNRegressor(random_state = rs, **architecture)\n",
    "    prediction += model.fit(X_train,y_train).predict(X_test)/rep\n",
    "    model.delete_model_weights()\n",
    "    del model\n",
    "print(r2(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 30 #number of NN\n",
    "architectures = [MLR1_parameters, MLR2_parameters, MLR3_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7801080298201324\n"
     ]
    }
   ],
   "source": [
    "prediction = 0.\n",
    "for rs in range(rep):\n",
    "    model = MLRNNRegressor(random_state = rs, **architectures[rs%len(architectures)])\n",
    "    prediction += model.fit(X_train,y_train).predict(X_test)/rep\n",
    "    model.delete_model_weights()\n",
    "    del model\n",
    "print(r2(y_test, prediction)) #MLR2 works best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select 5 best models among 30, based on valid performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 30 #number of NN\n",
    "selected = 5\n",
    "architectures = [MLR1_parameters, MLR2_parameters, MLR3_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "valid_performances = []\n",
    "for rs in range(rep):\n",
    "    model = MLRNNRegressor(random_state = rs, **architectures[rs%len(architectures)])\n",
    "    predictions.append(model.fit(X_train,y_train).predict(X_test))\n",
    "    valid_performances.append(np.max(model.record[\"validation\"]))\n",
    "    model.delete_model_weights()\n",
    "    del model\n",
    "prediction = np.mean([predictions[index] for index in np.argsort(valid_performances)[-selected:]],axis = 0)\n",
    "print(r2(y_test, prediction)) #MLR2 + ML1 works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8067195738836694\n"
     ]
    }
   ],
   "source": [
    "prediction = np.mean([predictions[index] for index in np.argsort(valid_performances)[-selected:]],axis = 0)\n",
    "print(r2(y_test, prediction)) #MLR2 + ML1 works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 30 out of 30 : 0.7801080303317011\n",
      "Best 29 out of 30 : 0.7811653673787027\n",
      "Best 28 out of 30 : 0.7812901033715316\n",
      "Best 27 out of 30 : 0.7816359446469722\n",
      "Best 26 out of 30 : 0.7832028999954317\n",
      "Best 25 out of 30 : 0.783497861675845\n",
      "Best 24 out of 30 : 0.7841571106428884\n",
      "Best 23 out of 30 : 0.7847603724028064\n",
      "Best 22 out of 30 : 0.7850970458906381\n",
      "Best 21 out of 30 : 0.7851721191668117\n",
      "Best 20 out of 30 : 0.7861790814027839\n",
      "Best 19 out of 30 : 0.787164450973215\n",
      "Best 18 out of 30 : 0.7876299596977294\n",
      "Best 17 out of 30 : 0.7880871941181802\n",
      "Best 16 out of 30 : 0.7898434473660129\n",
      "Best 15 out of 30 : 0.791577683624718\n",
      "Best 14 out of 30 : 0.7945700003943788\n",
      "Best 13 out of 30 : 0.7975168612469334\n",
      "Best 12 out of 30 : 0.7997296919946857\n",
      "Best 11 out of 30 : 0.8002942425756356\n",
      "Best 10 out of 30 : 0.8035459943248342\n",
      "Best 9 out of 30 : 0.8051612570820368\n",
      "Best 8 out of 30 : 0.8080579079716153\n",
      "Best 7 out of 30 : 0.8094397040287625\n",
      "Best 6 out of 30 : 0.8058504828712936\n",
      "Best 5 out of 30 : 0.8067195738836694\n",
      "Best 4 out of 30 : 0.8103813829227441\n",
      "Best 3 out of 30 : 0.808860381624854\n",
      "Best 2 out of 30 : 0.8013661456035476\n",
      "Best 1 out of 30 : 0.813379522126614\n"
     ]
    }
   ],
   "source": [
    "for _selected in range(rep,0,-1):\n",
    "    prediction = np.mean([predictions[index] for index in np.argsort(valid_performances)[-_selected:]],axis = 0)\n",
    "    print(\"Best \" + str(_selected) + \" out of \"+ str(rep)+ \" :\", r2(y_test, prediction)) #MLR2 + ML1 works well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HP you could tune to go even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordered by pertinence, list of somewhat sensible values\n",
    "tunable_parameters = {\n",
    "\"depth\":[1,2,3,4],\n",
    "\"width\": [256, 512, 1024, 2048, 4096],\n",
    "\"learning_rate\" : [1e-1, 3*1e-2, 1e-2, 3*1e-3, 1e-3, 3*1e-4, 1e-4, 3*1e-5],\n",
    "\"batch_size\": [1.,1,16,32,64,128,256,512,1024,2048,4096,4096*2],\n",
    "\"target_rotation_scale\" : [False, 0.05, 0.2, 0.5],\n",
    "\"ridge_init\" : np.geomspace(1e-1, 1e5, 50),\n",
    "\"n_permut\" : [False,16],\n",
    "\"label_noise_scale\" : [False, 0.01, 0.03, 0.05],\n",
    "\"should_stratify\" : [True,False],\n",
    "\"activation\" : [\"relu\",\"sigmoid\"],\n",
    "\"optimizer\" : [\"Adam\",\"Adadelta\"]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
