{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4bc11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn as sk\n",
    "import MLRNN as MLR\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.preprocessing import StandardScaler as normalize\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "#also torch, xgboost, catboost, lightgbm, fastai, see below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dffab2",
   "metadata": {},
   "source": [
    "# User inputs (check this section then run all cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81755e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if importlib.util.find_spec('torch') is None:\n",
    "    raise ImportError(\"MLR is implemented in torch here! => conda install -c pytorch pytorch\")\n",
    "xgboost_available = importlib.util.find_spec('xgboost') is not None #conda install -c conda-forge xgboost\n",
    "catboost_available = importlib.util.find_spec('catboost') is not None #conda install -c conda-forge catboost\n",
    "lgbm_available = importlib.util.find_spec('lightgbm') is not None #conda install -c conda-forge lightgbm\n",
    "mars_available = importlib.util.find_spec('pyearth') is not None #conda install -c conda-forge sklearn-contrib-py-earth\n",
    "fastai_available = importlib.util.find_spec('fastai') is not None #conda install -c fastai fastai\n",
    "#else excluded from the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_repository = \"../preprocessed_datasets/\"\n",
    "output_repository = \"outputs/\"\n",
    "\n",
    "regression_benchmark_output_file = \"regression_benchmark.csv\"\n",
    "classification_benchmark_output_file = \"classification_benchmark.csv\"\n",
    "ablation_study_output_file = \"ablation_study.csv\"\n",
    "dependance_study_output_file = \"dependance_study.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83608ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_datasets = np.arange(16) #16 for regression, 16 for classification\n",
    "study_datasets = [-1, 0, 1] #Id for Seoul Bike Sharing Demand (without target log-rescale), Concrete Slump Testâˆ’3, QSAR aquatic toxicity, \n",
    "benchmark_seeds = 10\n",
    "study_seeds = 100\n",
    "\n",
    "ensemble_components = [\"MLR1\",\"MLR2\"] #architectures used for meta-models            \n",
    "benchmark_bagging_reps = 10 #number of estimator in each bagging model\n",
    "benchmark_top_valid_cut = 5 #number of estimators aggregated when sorted by validation-set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8400b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_regression_benchmark = True\n",
    "run_classification_benchmark = True\n",
    "run_ablation_study = True\n",
    "run_dependance_study = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1cc42",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(dataset_id, name, repository):\n",
    "    return np.load(repository + name + str(dataset_id) + \".npy\")\n",
    "def prepare_dataset(dataset, train_size = 0.8, seed= False):\n",
    "    kwargs = {}\n",
    "    if seed or type(seed) == type(0):\n",
    "        kwargs[\"random_state\"] = seed\n",
    "    X, y = dataset[:, :-1], dataset[:, -1]\n",
    "    X = normalize().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = tts(X, y, train_size = train_size, **kwargs)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def get_dataset(dataset_id, name, repository, train_size = 0.8, seed = False):\n",
    "    return prepare_dataset(dataset_loader(dataset_id, name, repository), train_size = train_size, seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbe2cb",
   "metadata": {},
   "source": [
    "# Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(results, output_file, output_repository, metrics = [\"R2\"]):\n",
    "    import os\n",
    "    if output_file not in os.listdir(output_repository):\n",
    "        with open(output_repository + output_file, \"w\") as file:\n",
    "            file.write(\",\".join([\"id\",\"dataset\",\"seed\",\"category\", \"method\",\"time\"]+metrics))\n",
    "            file.close()\n",
    "    with open(output_repository + output_file, \"a\") as file:\n",
    "        file.write(\"\\n\"+\",\".join(map(str,results)))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618ac67",
   "metadata": {},
   "source": [
    "# Method evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(methods, datasets, input_name, input_repository, output_file, output_repository, seeds = 10, regression = True):\n",
    "    if regression: metrics = [\"R2\"]\n",
    "    else: metrics = [\"ACC\",\"AUC\"]\n",
    "    for dataset_id in datasets:\n",
    "        for seed in range(seeds):\n",
    "            X_train, X_test, y_train, y_test = get_dataset(dataset_id, input_name, input_repository, train_size = 0.8, seed = False)\n",
    "\n",
    "            for method_category, method_name, function in methods:\n",
    "                exp_id = str(dataset_id)+'_'+str(seed)+\"_\"+str(method_category)+\"_\"+str(method_name)\n",
    "                start_time = time.time()\n",
    "                results = eval(function)(X_train, X_test, y_train, y_test, method_name, seed, regression = regression)\n",
    "                end_time = time.time() - start_time\n",
    "                \n",
    "                result_line = [exp_id, dataset_id, seed, method_category, method_name, end_time]+results\n",
    "                write_results(result_line, output_file, output_repository, metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9d758",
   "metadata": {},
   "source": [
    "# Method categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_name = \"Baseline\"\n",
    "lm_name = \"GLM\"\n",
    "QDA_name = \"QDA\"\n",
    "tree_name = \"TREE\"\n",
    "ensemble_name = \"RF\"\n",
    "spline_name = \"MARS\"\n",
    "svm_name = \"SVM\"\n",
    "nn_name = \"NN\"\n",
    "xgb_name = \"GBDT\"\n",
    "mlr_name = \"MLR\"\n",
    "\n",
    "fastai_experiment = \"run_fastai\"\n",
    "xgb_experiment = \"run_xgb\"\n",
    "sklearn_experiment = \"run_sklearn\"\n",
    "mlr_experiment = \"run_mlr\"\n",
    "\n",
    "regressor_methods = [(baseline_name, \"Intercept\", sklearn_experiment),\n",
    "                     (lm_name, \"Ridge\", sklearn_experiment),\n",
    "                     (lm_name, \"Lasso\", sklearn_experiment),\n",
    "                     (lm_name, \"Enet\", sklearn_experiment),\n",
    "                     (tree_name, \"CART\", sklearn_experiment),\n",
    "                     (ensemble_name, \"RF\", sklearn_experiment),\n",
    "                     (ensemble_name, \"XRF\", sklearn_experiment),\n",
    "                     (xgb_name, \"xgb_sklearn\", sklearn_experiment),\n",
    "                     (svm_name, \"Kernel\", sklearn_experiment),\n",
    "                     (svm_name, \"NuSVM\", sklearn_experiment),\n",
    "                     (nn_name, \"MLP_sklearn\", sklearn_experiment),\n",
    "                     (mlr_name, \"MLR3\", mlr_experiment),\n",
    "                     (mlr_name, \"MLR4\", mlr_experiment)]\n",
    "regressor_methods += [(spline_name, \"MARS\", sklearn_experiment)] * mars_available\n",
    "regressor_methods += [(xgb_name, \"XGBoost\", xgb_experiment)] * xgboost_available\n",
    "regressor_methods += [(xgb_name, \"CAT\", xgb_experiment)] * catboost_available\n",
    "regressor_methods += [(xgb_name, \"LGBM\", xgb_experiment)] * lgbm_available\n",
    "regressor_methods += [(nn_name, \"fastai\", fastai_experiment)] * fastai_available     \n",
    "                    \n",
    "classifier_methods = [(baseline_name, \"Intercept\", sklearn_experiment),\n",
    "                     (lm_name, \"Ridge\", sklearn_experiment),\n",
    "                     (lm_name, \"LinearRidge\", sklearn_experiment),\n",
    "                     (lm_name, \"Lasso\", sklearn_experiment),\n",
    "                     (lm_name, \"Enet\", sklearn_experiment),\n",
    "                     (QDA_name, \"QDA\", sklearn_experiment),\n",
    "                     (tree_name, \"CART\", sklearn_experiment),\n",
    "                     (tree_name, \"XCART\", sklearn_experiment),\n",
    "                     (ensemble_name, \"RF\", sklearn_experiment),\n",
    "                     (ensemble_name, \"XRF\", sklearn_experiment),\n",
    "                     (xgb_name, \"xgb_sklearn\", sklearn_experiment),\n",
    "                     (xgb_name, \"ADABoost\", sklearn_experiment),\n",
    "                     (nn_name, \"MLP_sklearn\", sklearn_experiment),\n",
    "                     (mlr_name, \"MLR3\", mlr_experiment),\n",
    "                     (mlr_name, \"MLR4\", mlr_experiment)]\n",
    "classifier_methods += [(xgb_name, \"XGBoost\", xgb_experiment)] * xgboost_available\n",
    "classifier_methods += [(xgb_name, \"CAT\", xgb_experiment)] * catboost_available\n",
    "classifier_methods += [(xgb_name, \"LGBM\", xgb_experiment)] * lgbm_available\n",
    "classifier_methods += [(nn_name, \"fastai\", fastai_experiment)] * fastai_available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae68a19",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b41b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn\n",
    "deterministic_methods = [\"Intercept\",\n",
    "                         \"Ridge\",\n",
    "                         \"Lasso\",\n",
    "                         \"Enet\",\n",
    "                         \"LinearRidge\"\n",
    "                         \"CART\",\n",
    "                         \"XCART\",\n",
    "                         \"Kernel\",\n",
    "                         \"NuSVM\",\n",
    "                         \"MARS\",\n",
    "                         \"QDA\",\n",
    "                         \"LinearRidge\"]\n",
    "\n",
    "\n",
    "def run_sklearn(X_train, X_test, y_train, y_test, method_name, seed, regression = True):\n",
    "    if method_name in deterministic_methods: hyper_parameters = {}\n",
    "    else: hyper_parameters = {\"random_state\" : seed}   \n",
    "        \n",
    "    #rename sklearn classes such that classifier and regressor have the same name\n",
    "    if regression:\n",
    "        from sklearn.dummy import DummyRegressor as Intercept\n",
    "        from sklearn.linear_model import RidgeCV as Ridge\n",
    "        from sklearn.linear_model import LassoCV as Lasso\n",
    "        from sklearn.linear_model import ElasticNetCV as Enet\n",
    "        from sklearn.tree import DecisionTreeRegressor as CART\n",
    "        from sklearn.ensemble import RandomForestRegressor as RF\n",
    "        from sklearn.ensemble import ExtraTreesRegressor as XRF\n",
    "        from sklearn.ensemble import GradientBoostingRegressor as xgb_sklearn\n",
    "        from sklearn.kernel_ridge import KernelRidge as Kernel\n",
    "        from sklearn.svm import NuSVR as NuSVM\n",
    "        from sklearn.neural_network import MLPRegressor as MLP_sklearn\n",
    "        from pyearth import Earth as MARS\n",
    "        \n",
    "    else:                   \n",
    "        from functools import partial\n",
    "        from sklearn.calibration import CalibratedClassifierCV\n",
    "        from sklearn.dummy import DummyClassifier as Intercept\n",
    "        from sklearn.linear_model import LogisticRegressionCV as LogitCV\n",
    "        Ridge = partial(LogitCV,penalty = \"l2\")\n",
    "        Lasso = partial(LogitCV,penalty = \"l1\", solver = 'liblinear')\n",
    "        Enet = partial(LogitCV,penalty = \"elasticnet\", l1_ratios = [0.5,0.9,.95], solver = 'saga')\n",
    "        from sklearn.linear_model import RidgeClassifierCV\n",
    "        from sklearn.utils.extmath import softmax\n",
    "        class LinearRidge(RidgeClassifierCV):\n",
    "            def predict_proba(self, X):\n",
    "                d = self.decision_function(X)\n",
    "                d_2d = np.c_[-d, d]\n",
    "                return softmax(d_2d)\n",
    "        from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "        from sklearn.tree import DecisionTreeClassifier as CART\n",
    "        from sklearn.tree import ExtraTreeClassifier as XCART\n",
    "        from sklearn.ensemble import RandomForestClassifier as RF\n",
    "        from sklearn.ensemble import ExtraTreesClassifier as XRF\n",
    "        from sklearn.ensemble import BaggingClassifier as Bagging\n",
    "        from sklearn.ensemble import AdaBoostClassifier as ADABoost\n",
    "        from sklearn.ensemble import GradientBoostingClassifier as xgb_sklearn\n",
    "        from sklearn.neural_network import MLPClassifier as MLP_sklearn \n",
    "        \n",
    "    if regression:\n",
    "        result = eval(method_name)(**hyper_parameters).fit(X_train, y_train).score(X_test, y_test)\n",
    "        return [result]\n",
    "    else:\n",
    "        model = eval(method_name)(**hyper_parameters).fit(X_train,y_train)\n",
    "        result = model.score(X_test, y_test)\n",
    "        roc = auc(y_test, model.predict_proba(X_test)[:,1])\n",
    "        return [result, roc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7080ba4",
   "metadata": {},
   "source": [
    "## Fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fastai\n",
    "from fastai.tabular.all import *\n",
    "import pandas as pd\n",
    "from scipy.special import expit as logistic_func\n",
    "def run_fastai(X_train, X_test, y_train, y_test, method_name, seed, regression = True):\n",
    "    #no simple way to set random state\n",
    "    #forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628\n",
    "    df = pd.DataFrame(X_train)\n",
    "    df[\"target\"] = y_train\n",
    "    dls = TabularPandas(df, procs=[],\n",
    "                       cat_names = [],\n",
    "                       cont_names = range(len(df.columns)-1),\n",
    "                       y_names='target',\n",
    "                       splits=RandomSplitter(valid_pct=0.2)(range_of(df))).dataloaders(bs=64)\n",
    "    if regression:\n",
    "        learn = tabular_learner(dls, metrics=rmse)\n",
    "        learn.cbs = [learn.cbs[0]]\n",
    "        learn.fit_one_cycle(200)\n",
    "        result = r2_score(y_test, learn.get_preds(dl=learn.dls.test_dl(pd.DataFrame(X_test)))[0].numpy())\n",
    "        del learn\n",
    "        return [result]\n",
    "    else:\n",
    "        learn = tabular_learner(dls, metrics=accuracy)\n",
    "        learn.cbs = [learn.cbs[0]]\n",
    "        learn.fit_one_cycle(200)\n",
    "        decision = learn.get_preds(dl=learn.dls.test_dl(pd.DataFrame(X_test)))[0].numpy()\n",
    "        preds = (decision>0).astype(int)\n",
    "        result, roc = acc(y_test,preds), auc(y_test, logistic_func(decision))\n",
    "        return [result, roc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e337630",
   "metadata": {},
   "source": [
    "## GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost, catboost, lightgbm\n",
    "def run_xgb(X_train, X_test, y_train, y_test, method_name, seed, regression = True):\n",
    "    if regression:\n",
    "        if method_name == \"XGBoost\":\n",
    "            from xgboost import XGBRegressor as XGB\n",
    "            model = XGB(random_state = seed, objective ='reg:squarederror', verbose = False).fit(X_train,y_train)\n",
    "            result = model.score(X_test, y_test)\n",
    "\n",
    "        elif method_name == \"CAT\":\n",
    "            from catboost import CatBoostRegressor as CAT\n",
    "            model = CAT(random_seed=seed, logging_level='Silent').fit(X_train,y_train)\n",
    "            result = model.score(X_test, y_test)\n",
    "            \n",
    "        elif method_name == \"LGBM\":\n",
    "            from lightgbm.sklearn import LGBMRegressor as LGBM\n",
    "            model = LGBM(random_state=seed).fit(X_train,y_train)\n",
    "            result = r2_score(y_test, model.predict(X_test)) \n",
    "            \n",
    "        del model    \n",
    "        return [result]\n",
    "    \n",
    "    else:\n",
    "        if method_name == \"XGBoost\":\n",
    "            from xgboost import XGBClassifier as XGB\n",
    "            model = XGB(random_state = seed, verbose = False).fit(X_train,y_train)\n",
    "            result = model.score(X_test, y_test)\n",
    "            roc = auc(y_test, model.predict_proba(X_test)[:,1])\n",
    "\n",
    "        elif method_name == \"CAT\":\n",
    "            from catboost import CatBoostClassifier as CAT\n",
    "            model = CAT(random_seed=seed, logging_level='Silent').fit(X_train,y_train)\n",
    "            result = model.score(X_test, y_test)\n",
    "            roc = auc(y_test, model.predict_proba(X_test)[:,1])\n",
    "            \n",
    "        elif method_name == \"LGBM\":\n",
    "            from lightgbm.sklearn import LGBMClassifier as LGBM\n",
    "            model = LGBM(random_state=seed).fit(X_train,y_train)\n",
    "            result = acc(y_test, model.predict(X_test))\n",
    "            roc = auc(y_test, model.predict_proba(X_test)[:,1])\n",
    "            \n",
    "        del model\n",
    "        return [result, roc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718aa0c",
   "metadata": {},
   "source": [
    "## MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f34419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set architectures hyper-parameters\n",
    "common_parameters = {\"max_runtime\" : 900, \"width\":1024}\n",
    "MLR1_parameters = deepcopy(common_parameters)\n",
    "MLR1_parameters[\"depth\"] = 1\n",
    "MLR1_parameters[\"learning_rate\"] = 1e-2\n",
    "MLR1_parameters[\"max_iter\"] = 200\n",
    "\n",
    "MLR2_parameters = deepcopy(common_parameters)\n",
    "MLR2_parameters[\"depth\"] = 2\n",
    "MLR2_parameters[\"learning_rate\"] = 1e-3\n",
    "MLR2_parameters[\"max_iter\"] = 200\n",
    "\n",
    "MLR3_parameters = deepcopy(common_parameters)\n",
    "MLR3_parameters[\"depth\"] = 3\n",
    "MLR3_parameters[\"learning_rate\"] = 1e-3 /3\n",
    "MLR3_parameters[\"max_iter\"] = 400\n",
    "\n",
    "MLR4_parameters = deepcopy(common_parameters)\n",
    "MLR4_parameters[\"depth\"] = 4\n",
    "MLR4_parameters[\"learning_rate\"] = 1e-4\n",
    "MLR4_parameters[\"max_iter\"] = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlr(X_train, X_test, y_train, y_test, method_name, seed, regression = True):\n",
    "    if regression:\n",
    "        model = MLR.MLRNNRegressor(random_state = seed, **eval(method_name+\"_parameters\")).fit(X_train,y_train)\n",
    "        result = model.score(X_test, y_test)\n",
    "        model.delete_model_weights()\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        return [result]\n",
    "    else:\n",
    "        model = MLR.MLRNNClassifier(random_state = seed, **eval(method_name+\"_parameters\")).fit(X_train,y_train)\n",
    "        result = model.score(X_test, y_test)\n",
    "        roc = auc(y_test, model.predict_proba(X_test)[:,1])\n",
    "        model.delete_model_weights()\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        return [result, roc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de068c19",
   "metadata": {},
   "source": [
    "## MLR Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute aggregated models simultaneously, based on the same set of predictions\n",
    "#in our experiments, ensemble_components = [\"MLR1\",\"MLR2\"]\n",
    "\n",
    "def get_MLR_prediction(X_train, X_test, y_train, y_test, method_name, seed, regression = True):\n",
    "    #for one estimator, get the validation score and for each test-set observation the predicted value \n",
    "    if regression:\n",
    "        model = MLR.MLRNNRegressor(random_state = seed, **eval(method_name+\"_parameters\")).fit(X_train,y_train)\n",
    "        prediction = model.predict(X_test).reshape(-1)\n",
    "        \n",
    "    else:\n",
    "        model = MLR.MLRNNClassifier(random_state = seed, **eval(method_name+\"_parameters\")).fit(X_train,y_train)\n",
    "        prediction = model.predict_proba(X_test)[:,1]\n",
    "        \n",
    "    validation_performance = np.max(model.record[\"validation\"])\n",
    "    model.delete_model_weights()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return prediction, validation_performance\n",
    "\n",
    "def evaluate_MLR_prediction(y_test, prediction, exp_id, dataset_id, seed, method_category, method_name, end_time,output_file, output_repository, regression = True):\n",
    "    #for an aggregated prediction vector, compute score and write results\n",
    "    if regression:\n",
    "        result = r2_score(y_test, prediction) \n",
    "        result_line = [exp_id, dataset_id, seed, method_category, method_name, end_time]+[result]\n",
    "        write_results(result_line, output_file, output_repository, metrics = [\"R2\"])\n",
    "    else:\n",
    "        result = acc(y_test, prediction >= 0.5) \n",
    "        roc = auc(y_test, prediction) \n",
    "        result_line = [exp_id, dataset_id, seed, method_category, method_name, end_time]+[result, roc]\n",
    "        write_results(result_line, output_file, output_repository, metrics = [\"ACC\",\"AUC\"])\n",
    "    \n",
    "def run_ensemble(ensemble_components, \n",
    "                            datasets, \n",
    "                            input_name, \n",
    "                            input_repository, \n",
    "                            output_file, \n",
    "                            output_repository, \n",
    "                            seeds = 10, \n",
    "                            bagging_reps = 10,\n",
    "                            top_valid_cut = 5,\n",
    "                            regression = True):\n",
    "    method_category = \"MLR\"\n",
    "    for dataset_id in datasets:\n",
    "        for seed in range(seeds):\n",
    "            X_train, X_test, y_train, y_test = get_dataset(dataset_id, input_name, input_repository, seed = seed)\n",
    "            predictions = {}\n",
    "            validation_performances = {}\n",
    "            for method_name in ensemble_components:\n",
    "                predictions[method_name] = []\n",
    "                validation_performances[method_name] = []\n",
    "                exp_id = str(dataset_id)+'_'+str(seed)+\"_\"+str(method_category)+\"_\"+str(method_name)\n",
    "\n",
    "                start_time = time.time()\n",
    "                for rep in range(bagging_reps):\n",
    "                    prediction, validation_performance = get_MLR_prediction(X_train, X_test, y_train, y_test, method_name, rep + seed * bagging_reps, regression = True)\n",
    "                    predictions[method_name].append(prediction)\n",
    "                    validation_performances[method_name].append(validation_performance)\n",
    "\n",
    "                #Mean performance accross several models\n",
    "                if regression:\n",
    "                    result = np.mean([r2_score(y_test, pred) for pred in predictions[method_name]])\n",
    "                    end_time = (time.time()-start_time)/bagging_reps\n",
    "                    result_line = [exp_id, dataset_id, seed, method_category, method_name, end_time]+[result]\n",
    "                    write_results(result_line, output_file, output_repository, metrics = [\"R2\"])\n",
    "                else:\n",
    "                    result = np.mean([acc(y_test, pred >= 0.5) for pred in predictions[method_name]])\n",
    "                    roc = np.mean([auc(y_test, pred) for pred in predictions[method_name]])\n",
    "                    end_time = (time.time()-start_time)/bagging_reps\n",
    "                    result_line = [exp_id, dataset_id, seed, method_category, method_name, end_time]+[result, roc]\n",
    "                    write_results(result_line, output_file, output_repository, metrics = [\"ACC\",\"AUC\"])\n",
    "\n",
    "                #Bagging aggregation\n",
    "                prediction = np.mean(predictions[method_name], axis = 0)\n",
    "                method_name = \"Bagging_\" + method_name\n",
    "                end_time = 0\n",
    "                exp_id = str(dataset_id)+'_'+str(seed)+\"_\"+str(method_category)+\"_\"+str(method_name)\n",
    "                evaluate_MLR_prediction(y_test, prediction, exp_id, dataset_id, seed, method_category, method_name, end_time,output_file, output_repository, regression = regression)\n",
    "\n",
    "            #Ensemble aggregation\n",
    "            prediction = np.mean([np.mean(predictions[method_name], axis = 0) for method_name in ensemble_components], axis = 0)\n",
    "            method_name = \"ensemble\"\n",
    "            end_time = 0\n",
    "            exp_id = str(dataset_id)+'_'+str(seed)+\"_\"+str(method_category)+\"_\"+str(method_name)\n",
    "            evaluate_MLR_prediction(y_test, prediction, exp_id, dataset_id, seed, method_category, method_name, end_time,output_file, output_repository, regression = regression)\n",
    "\n",
    "            #Valid model selection\n",
    "            predictions = [prediction for method_name in ensemble_components for prediction in predictions[method_name]]\n",
    "            validation_performances = [validation_performance for method_name in ensemble_components for validation_performance in validation_performances[method_name]]\n",
    "            top_valid_performances = np.argsort(validation_performances)[-top_valid_cut:]\n",
    "\n",
    "            #Best model\n",
    "            method_name = \"Best-MLR\"  \n",
    "            end_time = 0\n",
    "            exp_id = str(dataset_id)+'_'+str(seed)+\"_\"+str(method_category)+\"_\"+str(method_name)\n",
    "            prediction = predictions[top_valid_performances[-1]]\n",
    "            evaluate_MLR_prediction(y_test, prediction, exp_id, dataset_id, seed, method_category, method_name, end_time,output_file, output_repository, regression = regression)\n",
    "\n",
    "            #Top top_valid_cut models\n",
    "            method_name = \"Top\"+str(top_valid_cut)+\"-MLR\"  \n",
    "            end_time = 0\n",
    "            exp_id = str(dataset_id)+'_'+str(seed)+\"_\"+str(method_category)+\"_\"+str(method_name)\n",
    "            prediction = np.mean([predictions[index] for index in top_valid_performances], axis = 0)\n",
    "            evaluate_MLR_prediction(y_test, prediction, exp_id, dataset_id, seed, method_category, method_name, end_time,output_file, output_repository, regression = regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5045356",
   "metadata": {},
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740bb93",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3533f8",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0845d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_regression_benchmark:\n",
    "    task_name = \"regression\"\n",
    "    regression = True\n",
    "\n",
    "    run_experiment(regressor_methods, benchmark_datasets, task_name, input_repository, regression_benchmark_output_file, output_repository, regression = True, seeds = benchmark_seeds)\n",
    "    run_ensemble(ensemble_components, benchmark_datasets, task_name, input_repository, regression_benchmark_output_file, output_repository, regression = True, seeds = benchmark_seeds, bagging_reps = benchmark_bagging_reps, top_valid_cut = benchmark_top_valid_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc4e52",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d7dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_classification_benchmark:\n",
    "    task_name = \"classification\"\n",
    "    regression = False\n",
    "\n",
    "    run_experiment(classifier_methods, benchmark_datasets, task_name, input_repository, classification_benchmark_output_file, output_repository, regression = False, seeds = benchmark_seeds)\n",
    "    run_ensemble(ensemble_components,  benchmark_datasets, task_name, input_repository, classification_benchmark_output_file, output_repository, regression = False, seeds = benchmark_seeds, bagging_reps = benchmark_bagging_reps, top_valid_cut = benchmark_top_valid_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d90d9",
   "metadata": {},
   "source": [
    "## Ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21386d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2_parameters = deepcopy(MLR2_parameters)\n",
    "NN2_parameters[\"ridge_init\"] = False\n",
    "NN2_parameters[\"n_permut\"] = False\n",
    "NN2_parameters[\"target_rotation_scale\"] = False\n",
    "\n",
    "NN2_Ridge_parameters = deepcopy(MLR2_parameters)\n",
    "NN2_Ridge_parameters[\"ridge_init\"] = \"max_variation\"\n",
    "NN2_Ridge_parameters[\"n_permut\"] = False\n",
    "NN2_Ridge_parameters[\"target_rotation_scale\"] = False\n",
    "\n",
    "NN2_Ridge_SD_parameters = deepcopy(MLR2_parameters)\n",
    "NN2_Ridge_SD_parameters[\"ridge_init\"] = \"max_variation\"\n",
    "NN2_Ridge_SD_parameters[\"n_permut\"] = False\n",
    "NN2_Ridge_SD_parameters[\"target_rotation_scale\"] = 0.5\n",
    "\n",
    "NN2_Ridge_Permut_parameters = deepcopy(MLR2_parameters)\n",
    "NN2_Ridge_Permut_parameters[\"ridge_init\"] = \"max_variation\"\n",
    "NN2_Ridge_Permut_parameters[\"n_permut\"] = 16\n",
    "NN2_Ridge_Permut_parameters[\"target_rotation_scale\"] = False\n",
    "ablation_compared_architectures = [\"NN2\", \"NN2_Ridge\", \"NN2_Ridge_Permut\", \"NN2_Ridge_SD\", \"MLR2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b426fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_ablation_study:\n",
    "    ablation_bagging_reps = 10\n",
    "    ablation_top_valid_cut = 2 #performs useless but costless aggregation between the different architectures (discarded when processing results)\n",
    "\n",
    "    task_name = \"regression\"\n",
    "    regression = task_name == \"regression\"\n",
    "\n",
    "    run_ensemble(ablation_compared_architectures, study_datasets, task_name, input_repository, ablation_study_output_file, output_repository, seeds = study_seeds, bagging_reps = ablation_bagging_reps, top_valid_cut = ablation_top_valid_cut, regression = regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd331b2",
   "metadata": {},
   "source": [
    "## Dependance study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dependance_mlr(method_name, parameter_name, values, datasets, input_name, input_repository, output_file, output_repository, seeds = 10):\n",
    "    method_category = \"MLR\"\n",
    "    for dataset_id in datasets:\n",
    "        for seed in range(seeds):\n",
    "            X_train, X_test, y_train, y_test = get_dataset(dataset_id, input_name, input_repository, train_size = 0.8, seed = False)    \n",
    "            for value in values:\n",
    "                exp_id = str(dataset_id)+'_'+str(seed)+\"_\"+str(method_category)+\"_\"+str(method_name)+\"_\"+str(parameter_name)\n",
    "                parameters = deepcopy(eval(method_name+\"_parameters\"))\n",
    "                parameters.update({parameter_name : value})\n",
    "                \n",
    "                start_time = time.time()\n",
    "                model = MLR.MLRNNRegressor(random_state = seed, **parameters).fit(X_train, y_train)\n",
    "                result = model.score(X_test, y_test)\n",
    "                best_iter = model.best_iter\n",
    "                valid_max = model.record[\"validation\"][model.best_iter]\n",
    "                lambda_init = model.record[\"lambda\"][0]\n",
    "                model.delete_model_weights()\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "                end_time = time.time() - start_time\n",
    "                \n",
    "                result_line = [exp_id, dataset_id, seed, method_category, method_name, end_time] + [parameter_name, value, result, best_iter, valid_max, lambda_init]\n",
    "                write_results(result_line, output_file, output_repository, metrics = [\"parameter_name\", \"value\", \"R2\", \"best_iter\", \"valid_max\", \"lambda_init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc68d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_dependance_study:\n",
    "    dependance_loops = { \"target_rotation_scale\" : [0.,1e-1,0.5,1,1.5], #the actual value is twice this (for legacy code reasons)\n",
    "    \"n_permut\" : [0,1,2,4,16,256,1024],\n",
    "    \"ridge_init\" : [1e-3,1e-1,1e1,1e3,1e5,1e7,1e9, \"max_variation\"],\n",
    "    \"label_noise_scale\" : [0.,1e-2, 1e-2*3, 1e-1,1e-1*3],\n",
    "    \"width\": [16,64,256,1024,4096]}\n",
    "    task_name = \"regression\"\n",
    "    regression = task_name == \"regression\"\n",
    "    method_name = \"MLR2\"\n",
    "\n",
    "    for parameter_name, values in dependance_loops.items():\n",
    "        run_dependance_mlr(method_name, parameter_name, values, study_datasets, task_name, input_repository, dependance_study_output_file, output_repository, seeds = study_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad86d12b",
   "metadata": {},
   "source": [
    "## Batch size dependance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14257e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dependance_batchsize_mlr(method_name, values, datasets, input_name, input_repository, output_file, output_repository, seeds = 10):\n",
    "    method_category = \"MLR\"\n",
    "    parameter_name = \"batch_size\"\n",
    "    for dataset_id in datasets:\n",
    "        for seed in range(seeds):\n",
    "            X_train, X_test, y_train, y_test = get_dataset(dataset_id, input_name, input_repository, train_size = 0.8, seed = False)    \n",
    "            n = X_train.shape[0]\n",
    "            if n < np.max(values):\n",
    "                dataset_values = [value for value in values if value < n] + [n]\n",
    "            else: \n",
    "                dataset_values = [value for value in values]\n",
    "            for value in values:\n",
    "                exp_id = str(dataset_id)+'_'+str(seed)+\"_\"+str(method_category)+\"_\"+str(method_name)+\"_\"+str(parameter_name)\n",
    "                parameters = deepcopy(eval(method_name+\"_parameters\"))\n",
    "                parameters.update({parameter_name : value})\n",
    "                \n",
    "                start_time = time.time()\n",
    "                model = MLR.MLRNNRegressor(random_state = seed, **parameters).fit(X_train, y_train)\n",
    "                result = model.score(X_test, y_test)\n",
    "                best_iter = model.best_iter\n",
    "                valid_max = model.record[\"validation\"][model.best_iter]\n",
    "                lambda_init = model.record[\"lambda\"][0]\n",
    "                model.delete_model_weights()\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "                end_time = time.time() - start_time\n",
    "                \n",
    "                result_line = [exp_id, dataset_id, seed, method_category, method_name, end_time] + [parameter_name, value, result, best_iter, valid_max, lambda_init]\n",
    "                write_results(result_line, output_file, output_repository, metrics = [\"parameter_name\", \"value\", \"R2\", \"best_iter\", \"valid_max\", \"lambda_init\", ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_dependance_study:\n",
    "    task_name = \"regression\"\n",
    "    regression = task_name == \"regression\"\n",
    "    method_name = \"MLR2\"\n",
    "    values = [1,16,32,64,128,256,512,1024,2048,4096,8192, 16384]\n",
    "    run_dependance_batchsize_mlr(method_name, values, study_datasets, task_name, input_repository, dependance_study_output_file, output_repository, seeds = study_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8aba3",
   "metadata": {},
   "source": [
    "# Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class_dic = {'fastai':\"NN\",\n",
    "    \"MLP_sklearn\":\"NN\", \n",
    "    'MLR1':\"MLR\", \n",
    "    'Bagging_MLR1':\"MLR\", \n",
    "    'MLR2':\"MLR\", \n",
    "    'Bagging_MLR2':\"MLR\",\n",
    "    'ensemble':\"MLR\", \n",
    "    'Best-MLR':\"MLR\", \n",
    "    'Top5-MLR':\"MLR\", \n",
    "    'MLR3':\"MLR\", \n",
    "    'MLR4':\"MLR\",\n",
    "    'XGBoost':\"GBDT\", \n",
    "    'CAT':\"GBDT\", \n",
    "    'LGBM':\"GBDT\",\n",
    "    'Ridge':\"LM\", \n",
    "    'Lasso':\"LM\", \n",
    "    'Enet':\"LM\",\n",
    "    'LinearRidge':\"LM\", \n",
    "    'QDA':\"QDA\", \n",
    "    'CART':\"TREE\", \n",
    "    'XCART':\"TREE\", \n",
    "    'RF':\"RF\", \n",
    "    'XRF':\"RF\", \n",
    "    'Bagging':\"RF\",\n",
    "    'ADABoost':\"GBDT\", \n",
    "    'xgb_sklearn':\"GBDT\", \n",
    "    'Intercept':\"Baseline\", \n",
    "    'MARS':\"MARS\", \n",
    "    \"NuSVM\":\"SVM\",\n",
    "    \"Kernel\":\"SVM\"}\n",
    "\n",
    "\n",
    "def get_result_table(result_file, metric, reference = \"ensemble\"):\n",
    "    q_values = [0.90,0.95,0.98]\n",
    "    kept_columns = [\"class\"] + [metric+col for col in [\"\",\"_std\", \"_rank\",\"_rank_std\",\"_PMA\",\"_PMA_std\"]]+[metric+\"_P\"+str(q) for q in q_values]\n",
    "    \n",
    "    df2 = pd.read_csv(result_file)\n",
    "    df2[\"class\"] = [class_dic[method] for method in df2[\"method\"].values]\n",
    "    \n",
    "    #Compute PMA\n",
    "    df_max = df2.groupby([\"dataset\",\"seed\"]).max()\n",
    "    df2.set_index([\"dataset\",\"seed\"],inplace = True)\n",
    "    df2[metric+\"_max\"] = df_max[metric]\n",
    "    df2.reset_index(inplace = True)\n",
    "    df2[metric+\"_PMA\"] = df2[metric]/df2[metric+\"_max\"]\n",
    "\n",
    "    #Compute P90, P95, P98\n",
    "    df_p = df2.groupby([\"dataset\",\"seed\",\"class\"]).max().reset_index()\n",
    "    for q in q_values:\n",
    "        df_p[metric+\"_P\"+str(q)] = (df_p[metric+\"_PMA\"] > q).astype(int)\n",
    "    df_p = df_p.groupby('class').mean()\n",
    "\n",
    "    #Use ensemble(MLR1+MLR2) results accross all seeds as a baseline to mesure standard deviation for all methods\n",
    "    ensemble_ref = df2[df2[\"method\"]==reference].set_index([\"dataset\",\"seed\"])\n",
    "    df2.set_index([\"dataset\",\"seed\"], inplace = True)\n",
    "    df2[metric+\"_ref\"] = ensemble_ref[metric]\n",
    "    df2[metric+\"_std\"] = df2[metric].values-df2[metric+\"_ref\"].values\n",
    "    df2.reset_index(inplace = True)\n",
    "    df_mean_seed = df2.groupby([\"dataset\",\"method\"]).mean().reset_index()\n",
    "    df_mean_seed[\"class\"] = [class_dic[method] for method in df_mean_seed[\"method\"].values]\n",
    "    df_mean_seed.sort_values([\"dataset\",\"class\",metric],inplace = True)\n",
    "    df_mean_seed_max_class = df_mean_seed.groupby([\"dataset\",\"class\"]).last().reset_index()\n",
    "    df_mean_seed_max_class.set_index(\"method\",inplace = True)\n",
    "    df_mean_seed_max_class[metric + \"_std\"] = df2.groupby([\"method\"]).std()[metric+\"_std\"]\n",
    "    df_mean_seed_max_class[metric+\"_PMA\"] = df2.groupby(\"method\").mean()[metric+\"_PMA\"]\n",
    "    df_mean_seed_max_class[metric+\"_PMA_std\"] = df2.groupby(\"method\").std()[metric+\"_PMA\"]\n",
    "    df_mean_seed_max_class.reset_index(inplace = True)\n",
    "    df_mean_seed_max_class_mean_ds = df_mean_seed_max_class.groupby([\"class\"]).mean().reset_index()\n",
    "    \n",
    "    #Compute Friedman Rank\n",
    "    df_rank = df2.groupby([\"dataset\",\"seed\",\"class\"]).max().reset_index().sort_values([\"dataset\",\"seed\",metric],ascending = False)\n",
    "    df_rank[metric+\"_rank\"]= np.arange(len(df_rank))%len(df_rank[\"class\"].unique()) +1\n",
    "    df_mean_seed_max_class_mean_ds.set_index(\"class\", inplace = True)\n",
    "    df_mean_seed_max_class_mean_ds[metric+\"_rank\"] = df_rank.groupby(\"class\").mean()[metric+\"_rank\"]\n",
    "    df_mean_seed_max_class_mean_ds[metric+\"_rank_std\"] = df_rank.groupby(\"class\").std()[metric+\"_rank\"]\n",
    "    for q in q_values:\n",
    "        df_mean_seed_max_class_mean_ds[metric+\"_P\"+str(q)] = df_p[metric+\"_P\"+str(q)]\n",
    "    df_mean_seed_max_class_mean_ds.reset_index(inplace = True)\n",
    "    \n",
    "    #return results with only usefull columns\n",
    "    return df_mean_seed_max_class_mean_ds[kept_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbed66e",
   "metadata": {},
   "source": [
    "## Regression Benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ee316",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_regression_benchmark:\n",
    "    metric = \"R2\"\n",
    "    result_file = regression_benchmark_output_file #\"regression_benchmark.csv\"\n",
    "    output_file = \"processed_\"+metric+\"_\"+result_file\n",
    "    get_result_table(output_repository+result_file, metric).to_csv(output_repository+output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314917b",
   "metadata": {},
   "source": [
    "## Classification Benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_classification_benchmark:\n",
    "    metric = \"ACC\"\n",
    "    result_file = classification_benchmark_output_file #\"classification_benchmark.csv\"\n",
    "    output_file = \"processed_\"+metric+\"_\"+result_file\n",
    "    get_result_table(output_repository+result_file, metric).to_csv(output_repository+output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7dc860",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_classification_benchmark:\n",
    "    metric = \"AUC\"\n",
    "    result_file = classification_benchmark_output_file #\"classification_benchmark.csv\"\n",
    "    output_file = \"processed_\"+metric+\"_\"+result_file\n",
    "    get_result_table(output_repository+result_file, metric).to_csv(output_repository+output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98fbe0",
   "metadata": {},
   "source": [
    "## Ablation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ablation_results(result_file, metric, reference = 'Bagging_MLR2'):\n",
    "    df = pd.read_csv(result_file)\n",
    "    \n",
    "    #keep only single and bagging estimators (i.e. no ensemble)\n",
    "    kept_methods = [prefix + method_name  for method_name in [\"NN2\", \"NN2_Ridge\", \"NN2_Ridge_Permut\", \"NN2_Ridge_SD\", \"MLR2\"] for prefix in [\"\", \"Bagging_\"]]\n",
    "    df = df[np.isin(df[\"method\"].values,kept_methods)]\n",
    "    \n",
    "    #use MLR2_Bagging as a reference to compute result variation accross all seeds\n",
    "    df.set_index([\"dataset\",\"seed\"], inplace=True)\n",
    "    df[\"ref\"] = df[df[\"method\"] == reference][metric]\n",
    "    df.reset_index(inplace=True)\n",
    "    df['std'] = df[metric] - df[\"ref\"]\n",
    "    \n",
    "    #Average accross all seeds\n",
    "    mean_df = df.groupby([\"method\"]).mean()\n",
    "    std_df = df.groupby([\"method\"]).std()\n",
    "    mean_df[\"std\"] = std_df[\"std\"]\n",
    "    return mean_df.reset_index()[[\"method\", metric, \"std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af662fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_ablation_study:\n",
    "    result_file = ablation_study_output_file #\"ablation_study.csv\"\n",
    "    output_file = \"processed_\" + result_file\n",
    "    metric = \"R2\"\n",
    "    process_ablation_results(output_repository+result_file, metric).to_csv(output_repository+output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b460468",
   "metadata": {},
   "source": [
    "## Dependance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_dependance_study:\n",
    "    result_file = dependance_study_output_file #\"dependance_study.csv\"\n",
    "    output_file = \"processed_\" + result_file\n",
    "    kept_columns = [\"R2\", \"time\", \"best_iter\", \"valid_max\", \"lambda_init\"]\n",
    "    file = pd.read_csv(output_repository+result_file,delimiter = \",\").groupby([\"parameter_name\",\"dataset\",\"value\"]).mean()[kept_columns].to_csv(output_repository+output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
